{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba  #分词\n",
    "import pandas as pd #处理dataframe\n",
    "from gensim.models import Word2Vec #文本量化，词语-->向量\n",
    "from sklearn.linear_model import LinearRegression #线性回归运算\n",
    "from sklearn.model_selection import train_test_split #分训练和验证data\n",
    "import matplotlib.pyplot as plt #画图\n",
    "import numpy as np #代数运算\n",
    "import collections\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kw</th>\n",
       "      <th>logImp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5折机票</td>\n",
       "      <td>2.302585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1机票</td>\n",
       "      <td>1.609438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1机票查询</td>\n",
       "      <td>1.791759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1特价机票</td>\n",
       "      <td>1.568616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10月份特价机票</td>\n",
       "      <td>2.036882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4836</th>\n",
       "      <td>最低折扣机票</td>\n",
       "      <td>3.786373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4837</th>\n",
       "      <td>最低折扣机票查询</td>\n",
       "      <td>2.826656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4838</th>\n",
       "      <td>最好的机票网站</td>\n",
       "      <td>1.845827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4839</th>\n",
       "      <td>最好的机票预订网站</td>\n",
       "      <td>1.386294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4840</th>\n",
       "      <td>最优惠机票</td>\n",
       "      <td>1.945910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4841 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             kw    logImp\n",
       "0        1.5折机票  2.302585\n",
       "1        10.1机票  1.609438\n",
       "2      10.1机票查询  1.791759\n",
       "3      10.1特价机票  1.568616\n",
       "4      10月份特价机票  2.036882\n",
       "...         ...       ...\n",
       "4836     最低折扣机票  3.786373\n",
       "4837   最低折扣机票查询  2.826656\n",
       "4838    最好的机票网站  1.845827\n",
       "4839  最好的机票预订网站  1.386294\n",
       "4840      最优惠机票  1.945910\n",
       "\n",
       "[4841 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEM=pd.read_csv(\"/clubear/Lecture 6.3 - NLP - SEM Keyword Prediction/SEM.csv\")\n",
    "SEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分词和分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.798 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "wdtrain=[];wdlist=[]\n",
    "for kw in SEM.kw: #按照每一行（每一个样本）用jieba进行分词\n",
    "    wd=jieba.lcut(kw)\n",
    "    wdtrain.append(wd) #append是list\n",
    "    wdlist=wdlist+wd #+是直接加在后面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到的wdtrain是一个包含每个样本量分词的list的list， wdlist是一个所有分词的集合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab=collections.Counter(wdlist) #找出每个分词出现的次数并以dictionary的方式储存\n",
    "df = pd.DataFrame.from_dict(tab,orient=\"index\").reset_index() #重新设置index\n",
    "df = df.rename(columns={'index':'kw', 0:'count'}) #添加count column,重新以kw为index\n",
    "df = df.sort_values(by='count',ascending=False) #按照分词出现的数量进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'机票': 0,\n",
       " '飞机票': 1,\n",
       " '特价机票': 2,\n",
       " '查询': 3,\n",
       " '预订': 4,\n",
       " '便宜': 5,\n",
       " '-': 6,\n",
       " '打折': 7,\n",
       " '特价': 8,\n",
       " '到': 9,\n",
       " '深圳': 10,\n",
       " '北京': 11,\n",
       " '航班': 12,\n",
       " '广州': 13,\n",
       " '的': 14,\n",
       " '上海': 15,\n",
       " '网站': 16,\n",
       " '预定': 17,\n",
       " '网': 18,\n",
       " '订': 19,\n",
       " '飞机': 20,\n",
       " '网上': 21,\n",
       " '订机票': 22,\n",
       " '机票价格': 23,\n",
       " '三亚': 24,\n",
       " '价格': 25,\n",
       " '订票': 26,\n",
       " '买': 27,\n",
       " '十一': 28,\n",
       " '五一': 29,\n",
       " '折扣': 30,\n",
       " '南京': 31,\n",
       " '厦门': 32,\n",
       " '订购': 33,\n",
       " '重庆': 34,\n",
       " '武汉': 35,\n",
       " '天津': 36,\n",
       " '海口': 37,\n",
       " '长沙': 38,\n",
       " '昆明': 39,\n",
       " '去': 40,\n",
       " '最': 41,\n",
       " '哈尔滨': 42,\n",
       " '多少': 43,\n",
       " '定': 44,\n",
       " '大连': 45,\n",
       " '乌鲁木齐': 46,\n",
       " '钱': 47,\n",
       " '哪个': 48,\n",
       " '杭州': 49,\n",
       " '哪里': 50,\n",
       " '旅游': 51,\n",
       " '郑州': 52,\n",
       " '长春': 53,\n",
       " '官网': 54,\n",
       " '南宁': 55,\n",
       " '济南': 56,\n",
       " '电话': 57,\n",
       " '呼和浩特': 58,\n",
       " '机票网': 59,\n",
       " '成都': 60,\n",
       " '西安': 61,\n",
       " '南昌': 62,\n",
       " '票价': 63,\n",
       " '时刻表': 64,\n",
       " '珠海': 65,\n",
       " '烟台': 66,\n",
       " '春节': 67,\n",
       " '低价': 68,\n",
       " '太原': 69,\n",
       " '端午节': 70,\n",
       " '北京机票': 71,\n",
       " '自助游': 72,\n",
       " '什么': 73,\n",
       " '西双版纳': 74,\n",
       " '沈阳': 75,\n",
       " '廉价': 76,\n",
       " '往返机票': 77,\n",
       " '哪儿': 78,\n",
       " '青岛': 79,\n",
       " '攻略': 80,\n",
       " '好': 81,\n",
       " '怎么': 82,\n",
       " '成都机票': 83,\n",
       " '放假': 84,\n",
       " '携程网': 85,\n",
       " '优惠机票': 86,\n",
       " '2013': 87,\n",
       " '西安机票': 88,\n",
       " '如何': 89,\n",
       " '黄山': 90,\n",
       " '吗': 91,\n",
       " '元旦': 92,\n",
       " '往返': 93,\n",
       " '怎样': 94,\n",
       " '哪': 95,\n",
       " '携程': 96,\n",
       " '头等舱': 97,\n",
       " '贵阳': 98,\n",
       " '优惠': 99,\n",
       " '丽江': 100,\n",
       " '九寨沟': 101,\n",
       " '张家界': 102,\n",
       " '期间': 103,\n",
       " '购买': 104,\n",
       " '机票价': 105,\n",
       " '酷讯': 106,\n",
       " '最低': 107,\n",
       " '提前': 108,\n",
       " '航空': 109,\n",
       " '电子机票': 110,\n",
       " '温州': 111,\n",
       " '机场': 112,\n",
       " '在': 113,\n",
       " '特惠': 114,\n",
       " '年': 115,\n",
       " '南航': 116,\n",
       " '团购': 117,\n",
       " '国内': 118,\n",
       " '桂林': 119,\n",
       " '时候': 120,\n",
       " '艺龙': 121,\n",
       " '折': 122,\n",
       " '哪网': 123,\n",
       " '途牛': 124,\n",
       " '同程': 125,\n",
       " '热线': 126,\n",
       " '同程网': 127,\n",
       " '航空公司': 128,\n",
       " '5.1': 129,\n",
       " '石家庄': 130,\n",
       " '那个': 131,\n",
       " '定票': 132,\n",
       " '途牛网': 133,\n",
       " '多久': 134,\n",
       " '1': 135,\n",
       " '国内机票': 136,\n",
       " '北海': 137,\n",
       " '网订': 138,\n",
       " '一折': 139,\n",
       " '合肥': 140,\n",
       " '官方网站': 141,\n",
       " '最好': 142,\n",
       " '信用卡': 143,\n",
       " '安排': 144,\n",
       " '牡丹江': 145,\n",
       " '南方': 146,\n",
       " '银川': 147,\n",
       " '宁波': 148,\n",
       " '徐州': 149,\n",
       " '兰州': 150,\n",
       " '月份': 151,\n",
       " '拉萨': 152,\n",
       " '九江': 153,\n",
       " '绵阳': 154,\n",
       " '查': 155,\n",
       " '义乌': 156,\n",
       " '泸州': 157,\n",
       " '旅游网': 158,\n",
       " '儿童': 159,\n",
       " '南阳': 160,\n",
       " '福州': 161,\n",
       " '常州': 162,\n",
       " '全国': 163,\n",
       " '西宁': 164,\n",
       " '包头': 165,\n",
       " '哪家': 166,\n",
       " '临沂': 167,\n",
       " '艺龙网': 168,\n",
       " '酷讯网': 169,\n",
       " '推荐': 170,\n",
       " '庆阳': 171,\n",
       " '阜阳': 172,\n",
       " '5': 173,\n",
       " '宜昌': 174,\n",
       " '民航': 175,\n",
       " '海南': 176,\n",
       " '榆林': 177,\n",
       " '南苑': 178,\n",
       " '票': 179,\n",
       " '衢州': 180,\n",
       " '天气预报': 181,\n",
       " '海拉尔': 182,\n",
       " '旅游景点': 183,\n",
       " '旅行网': 184,\n",
       " '柳州': 185,\n",
       " '儿童票': 186,\n",
       " '可靠': 187,\n",
       " '威海': 188,\n",
       " '无锡': 189,\n",
       " '广州白云机场': 190,\n",
       " '通辽': 191,\n",
       " '敦煌': 192,\n",
       " '安徽': 193,\n",
       " '可以': 194,\n",
       " '天气': 195,\n",
       " '延安': 196,\n",
       " '有': 197,\n",
       " '乌兰浩特': 198,\n",
       " '喀什': 199,\n",
       " '库尔勒': 200,\n",
       " '大理': 201,\n",
       " '湛江': 202,\n",
       " '查寻': 203,\n",
       " '武夷山': 204,\n",
       " '中国': 205,\n",
       " '适合': 206,\n",
       " '丹东': 207,\n",
       " '门票': 208,\n",
       " '出发': 209,\n",
       " '汕头': 210,\n",
       " '团购网': 211,\n",
       " '延吉': 212,\n",
       " '景德镇': 213,\n",
       " '揭阳': 214,\n",
       " '云南': 215,\n",
       " '会': 216,\n",
       " '晋江': 217,\n",
       " '真的': 218,\n",
       " '南通': 219,\n",
       " '阿克苏': 220,\n",
       " '佳木斯': 221,\n",
       " '宜宾': 222,\n",
       " '赤峰': 223,\n",
       " '盐城': 224,\n",
       " '梅州': 225,\n",
       " '小孩': 226,\n",
       " '广元': 227,\n",
       " '洛阳': 228,\n",
       " '南充': 229,\n",
       " '促销': 230,\n",
       " '潍坊': 231,\n",
       " '最低价': 232,\n",
       " '降价': 233,\n",
       " '查找': 234,\n",
       " '安全': 235,\n",
       " '海航': 236,\n",
       " '安庆': 237,\n",
       " '抢': 238,\n",
       " '路线': 239,\n",
       " '哪买': 240,\n",
       " '和田': 241,\n",
       " '购票': 242,\n",
       " '梧州': 243,\n",
       " '格尔木': 244,\n",
       " '购': 245,\n",
       " '乌海': 246,\n",
       " '好去处': 247,\n",
       " '比较': 248,\n",
       " '湖北': 249,\n",
       " '一周': 250,\n",
       " '黑河': 251,\n",
       " '过年': 252,\n",
       " '贵州': 253,\n",
       " '吉林': 254,\n",
       " '驴友': 255,\n",
       " '齐齐哈尔': 256,\n",
       " '机票信息': 257,\n",
       " '林芝': 258,\n",
       " '贵': 259,\n",
       " '襄樊': 260,\n",
       " '黄花': 261,\n",
       " '常德': 262,\n",
       " '中心': 263,\n",
       " '系统': 264,\n",
       " '今年': 265,\n",
       " '连云港': 266,\n",
       " '芒市': 267,\n",
       " '悠哉': 268,\n",
       " '网定': 269,\n",
       " '网买': 270,\n",
       " '达州': 271,\n",
       " '穷游网': 272,\n",
       " '保山': 273,\n",
       " '锡林浩特': 274,\n",
       " '秦皇岛': 275,\n",
       " '6': 276,\n",
       " '锦州': 277,\n",
       " '黄金周': 278,\n",
       " '正规': 279,\n",
       " '坐飞机': 280,\n",
       " '恩施': 281,\n",
       " '涨价': 282,\n",
       " '2': 283,\n",
       " '思茅': 284,\n",
       " '五一节': 285,\n",
       " '井冈山': 286,\n",
       " '10.1': 287,\n",
       " '东营': 288,\n",
       " '穷游': 289,\n",
       " '那儿': 290,\n",
       " '游': 291,\n",
       " '一日': 292,\n",
       " '抢购': 293,\n",
       " '大全': 294,\n",
       " '要': 295,\n",
       " '自由': 296,\n",
       " '塔城': 297,\n",
       " '行': 298,\n",
       " '地方': 299,\n",
       " '青海': 300,\n",
       " '四川': 301,\n",
       " '越': 302,\n",
       " '网是': 303,\n",
       " '飞机场': 304,\n",
       " '人少': 305,\n",
       " '旅行': 306,\n",
       " '秒杀': 307,\n",
       " '假期': 308,\n",
       " '最佳': 309,\n",
       " '是': 310,\n",
       " '国庆节': 311,\n",
       " '前': 312,\n",
       " '小': 313,\n",
       " '长假': 314,\n",
       " '时间': 315,\n",
       " '攀枝花': 316,\n",
       " '几折': 317,\n",
       " '澳门': 318,\n",
       " '哪定': 319,\n",
       " '伊宁': 320,\n",
       " '婴儿': 321,\n",
       " '几天': 322,\n",
       " '特价网': 323,\n",
       " '那里': 324,\n",
       " '加格达奇': 325,\n",
       " '算': 326,\n",
       " '白云机场': 327,\n",
       " '标准': 328,\n",
       " '达县': 329,\n",
       " '打折票': 330,\n",
       " '半': 331,\n",
       " '价钱': 332,\n",
       " '州': 333,\n",
       " '到达': 334,\n",
       " '东方航空': 335,\n",
       " '全攻略': 336,\n",
       " '朝阳': 337,\n",
       " '哪些': 338,\n",
       " '流程': 339,\n",
       " '安康': 340,\n",
       " '福建': 341,\n",
       " '西昌': 342,\n",
       " '甘肃': 343,\n",
       " '怎么样': 344,\n",
       " '中秋': 345,\n",
       " '虹桥机场': 346,\n",
       " '黄岩': 347,\n",
       " '河南': 348,\n",
       " '河北': 349,\n",
       " '浙江': 350,\n",
       " '新疆': 351,\n",
       " '天天': 352,\n",
       " '迪庆': 353,\n",
       " '咸阳': 354,\n",
       " '月': 355,\n",
       " '网购': 356,\n",
       " '池州': 357,\n",
       " '永州': 358,\n",
       " '广州白云国际机场': 359,\n",
       " '赣州': 360,\n",
       " '阿勒泰': 361,\n",
       " '剩余': 362,\n",
       " '短途': 363,\n",
       " '淘': 364,\n",
       " '万县': 365,\n",
       " '淮安': 366,\n",
       " '艺龙定': 367,\n",
       " '大同': 368,\n",
       " '深航': 369,\n",
       " '周边': 370,\n",
       " '介绍': 371,\n",
       " '兴义': 372,\n",
       " '国际': 373,\n",
       " '折价': 374,\n",
       " '宜春': 375,\n",
       " '国航': 376,\n",
       " '一元': 377,\n",
       " '五月': 378,\n",
       " '学生': 379,\n",
       " '专题': 380,\n",
       " '格': 381,\n",
       " '起': 382,\n",
       " '多长时间': 383,\n",
       " '搜': 384,\n",
       " '自助': 385,\n",
       " '云南旅游': 386,\n",
       " '十一月份': 387,\n",
       " '同城网': 388,\n",
       " '网络': 389,\n",
       " '平台': 390,\n",
       " '去处': 391,\n",
       " '网易': 392,\n",
       " '飞': 393,\n",
       " '旅游胜地': 394,\n",
       " '干什么': 395,\n",
       " '报价': 396,\n",
       " '花费': 397,\n",
       " '打几折': 398,\n",
       " '活动': 399,\n",
       " '出行': 400,\n",
       " '前后': 401,\n",
       " '淘宝': 402,\n",
       " '海边': 403,\n",
       " '西塘': 404,\n",
       " '国庆': 405,\n",
       " '贤贤网': 406,\n",
       " '玩': 407,\n",
       " '查询电话': 408,\n",
       " '1.5': 409,\n",
       " '规定': 410,\n",
       " '山西': 411,\n",
       " '淡季': 412,\n",
       " '春运': 413,\n",
       " '改签': 414,\n",
       " '退': 415,\n",
       " '认证': 416,\n",
       " '代': 417,\n",
       " '网址': 418,\n",
       " '东北': 419,\n",
       " '东莞': 420,\n",
       " '二折': 421,\n",
       " '预订电话': 422,\n",
       " '超低价': 423,\n",
       " '余票': 424,\n",
       " '关于': 425,\n",
       " '广交会': 426,\n",
       " '广西': 427,\n",
       " '嘉峪关': 428,\n",
       " '扬州': 429,\n",
       " '遵义': 430,\n",
       " '邯郸': 431,\n",
       " '济宁': 432,\n",
       " '超': 433,\n",
       " '超低': 434,\n",
       " '国外': 435,\n",
       " 'jipiao': 436,\n",
       " '10': 437,\n",
       " '17u': 438,\n",
       " '.': 439,\n",
       " 'cn': 440,\n",
       " '元': 441,\n",
       " '51': 442,\n",
       " 'e': 443,\n",
       " '龙': 444,\n",
       " 'feijipiao': 445,\n",
       " 'tejiajipiao': 446,\n",
       " '已': 447,\n",
       " '遨游': 448,\n",
       " '白菜价': 449,\n",
       " '阿尔山': 450,\n",
       " '佛山': 451,\n",
       " '鸡西': 452,\n",
       " '长治': 453,\n",
       " '两日': 454,\n",
       " '北京首都机场': 455,\n",
       " '伊春': 456,\n",
       " '在线': 457,\n",
       " '黑龙江': 458,\n",
       " '厦门航空': 459,\n",
       " '宁夏': 460,\n",
       " '预约': 461,\n",
       " '历史': 462,\n",
       " '两折': 463,\n",
       " '辽宁': 464,\n",
       " '临沧': 465,\n",
       " '售票': 466,\n",
       " '哪有': 467,\n",
       " '那拉提': 468,\n",
       " '南方航空公司': 469,\n",
       " '平价': 470,\n",
       " '七天': 471,\n",
       " '青春': 472,\n",
       " '机': 473,\n",
       " '那网': 474,\n",
       " '能': 475,\n",
       " '三峡': 476,\n",
       " '凤凰': 477,\n",
       " '三折': 478,\n",
       " '厦航': 479,\n",
       " '厦门航空公司': 480,\n",
       " '库车': 481,\n",
       " '京东': 482,\n",
       " '湖南': 483,\n",
       " '卡': 484,\n",
       " '风景区': 485,\n",
       " '注意事项': 486,\n",
       " '亲子': 487,\n",
       " '惠州': 488,\n",
       " '比价': 489,\n",
       " '采购': 490,\n",
       " '东航': 491,\n",
       " '呢': 492,\n",
       " '及': 493,\n",
       " '定购': 494,\n",
       " '江西': 495,\n",
       " '附加费': 496,\n",
       " '号': 497,\n",
       " '货到付款': 498,\n",
       " '酒店': 499,\n",
       " '票号': 500,\n",
       " '搜索': 501,\n",
       " '可信': 502,\n",
       " '早订': 503,\n",
       " '江苏': 504,\n",
       " '舟山': 505}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncov=len(df.kw) #分词数量\n",
    "topkw=list(df.kw) #所有分词\n",
    "mydict=dict.fromkeys(topkw)  #会造一个value是空的字典\n",
    "for i in range(ncov):\n",
    "    mydict[topkw[i]]=i  #在字典分词变为整数，从0开始到ncov结束\n",
    "mydict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=len(SEM.kw) #sample size 样本量\n",
    "ncov=len(mydict) #分词数量\n",
    "X=np.zeros([ss,ncov+1]) #生成一个X的0-1矩阵。如果一个关键词包含相应词根，那么对应的X元素取值为1，否则为0\n",
    "for i in range(ss):\n",
    "    columnID=itemgetter(*wdtrain[i])(mydict)\n",
    "    X[i,columnID]=1\n",
    "    X[i,ncov]=len(SEM.kw[i])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "506"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncov\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 6.],\n",
       "       [1., 0., 0., ..., 0., 0., 6.],\n",
       "       [1., 0., 0., ..., 0., 0., 8.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 7.],\n",
       "       [1., 0., 0., ..., 0., 0., 9.],\n",
       "       [0., 0., 0., ..., 0., 0., 5.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "DIM=100 \n",
    "#然后做一个word2vec模型\n",
    "wvmodel = Word2Vec(wdtrain, size=DIM, min_count=1) # size表示词向量的大小，min_count表示最少的词频，低于此词频的词会被忽略掉\n",
    "Z=np.zeros([ss,DIM])\n",
    "for i in range(ss):\n",
    "    vec=wvmodel[wdtrain[i]]\n",
    "    Z[i,:]=np.mean(vec,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归模型分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把word2vect学习出来的空间坐标的平均值作为一个新的解释性变量Z与前面的X整合，形成新的矩阵XZ，然后做Y对XZ的回归分析，并检测外样本预测精度，外样本的预测精度其实是非常不稳定的，充满了异常值。因此，没有用均值，而是采用的中位数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.96"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XZ=np.hstack((X,Z))\n",
    "Y=np.array(SEM.logImp)\n",
    "Y=(Y-np.mean(Y))/np.std(Y)\n",
    "niter=100 #实验重复100次\n",
    "R21=np.zeros(niter)\n",
    "regmodel = LinearRegression() #先使用普通的线性回归模型\n",
    "for i in range(niter):\n",
    "    XZ0,XZ1,Y0,Y1= train_test_split(XZ,Y,test_size=0.5,random_state=i) #0%数据做训练，50%数据做验证\n",
    "    regmodel.fit(XZ0,Y0)\n",
    "    Yhat=regmodel.predict(XZ1)\n",
    "    R21[i]=100.0*(1-np.median((Yhat-Y1)**2)) \n",
    "np.round(np.median(R21),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以得到线性回归模型的MedSE的中位数是74.96\n",
    "## 用深度学习模型\n",
    "从网上了解到Sequential比较适合做Text分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 100)         50600     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, None, 128)         64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 116,029\n",
      "Trainable params: 116,029\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "embedding_dim = 100\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(layers.Embedding(ncov, embedding_dim))\n",
    "model2.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model2.add(layers.GlobalMaxPooling1D())\n",
    "model2.add(layers.Dense(10, activation='relu'))\n",
    "model2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model2.summary()\n",
    "model2.save(\"./home\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding的参数：100 * 506 = embedding_dim * ncov<br>\n",
    "Conv1D的参数：(100 * 5 +1）* 128 = 501 * 128 = 64128 , 其中5是kernel size，100是上一层遗留通道数，128是卷积核个数。 <br>\n",
    "Dense1的参数：128 * 10 + 10 = 1290<br>\n",
    "Dense2的参数: 10 * 1 + 1 = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model2 = keras.models.load_model('./home')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 4s 2ms/sample - loss: 0.1422 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 299us/sample - loss: -0.2241 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 309us/sample - loss: -2.1453 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 309us/sample - loss: -20.3294 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 301us/sample - loss: -86.2881 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 306us/sample - loss: -275.2490 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 310us/sample - loss: -592.9001 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 314us/sample - loss: -1459.5740 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 303us/sample - loss: -2675.3488 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 308us/sample - loss: -4827.9392 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 269us/sample - loss: -7195.8147 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 280us/sample - loss: -12798.8495 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 283us/sample - loss: -16477.9034 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 278us/sample - loss: -21666.1588 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 300us/sample - loss: -30427.3866 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 273us/sample - loss: -40174.3283 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 299us/sample - loss: -56592.7330 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 274us/sample - loss: -68168.9170 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 267us/sample - loss: -82408.8161 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 264us/sample - loss: -114812.8285 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 274us/sample - loss: -132728.2280 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 270us/sample - loss: -162532.2938 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 266us/sample - loss: -163027.1854 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 278us/sample - loss: -159519.4524 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 298us/sample - loss: -263410.9816 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 263us/sample - loss: -283446.1252 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 262us/sample - loss: -288835.1571 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 272us/sample - loss: -336598.5711 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 274us/sample - loss: -442424.0449 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 280us/sample - loss: -476761.7679 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 280us/sample - loss: -569038.7864 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 288us/sample - loss: -592918.1359 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 286us/sample - loss: -582208.4531 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 282us/sample - loss: -683082.0981 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 278us/sample - loss: -865356.1541 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 261us/sample - loss: -882697.0492 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 256us/sample - loss: -841008.2380 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 230us/sample - loss: -1042488.1709 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 238us/sample - loss: -1315850.9246 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 264us/sample - loss: -1348830.6415 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 276us/sample - loss: -1297825.3546 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 273us/sample - loss: -1440823.5899 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 289us/sample - loss: -1417456.6202 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 278us/sample - loss: -1612474.8074 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 269us/sample - loss: -1602679.9050 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 275us/sample - loss: -1786971.2169 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 260us/sample - loss: -2262296.8694 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 271us/sample - loss: -2469783.4539 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 259us/sample - loss: -2491586.0519 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 229us/sample - loss: -2453111.8771 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 274us/sample - loss: -2355688.0814 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 279us/sample - loss: -3113173.3420 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 287us/sample - loss: -3060574.4169 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 269us/sample - loss: -3427677.5810 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 282us/sample - loss: -3390932.2955 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 280us/sample - loss: -3355948.7434 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 260us/sample - loss: -4243514.1620 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 244us/sample - loss: -3973445.6901 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 254us/sample - loss: -4414929.3599 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 275us/sample - loss: -4359079.6131 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 274us/sample - loss: -5116393.5140 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 264us/sample - loss: -4707355.6826 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 277us/sample - loss: -6408338.6368 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 262us/sample - loss: -5923625.6702 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 278us/sample - loss: -6103975.9537 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 287us/sample - loss: -6208312.5831 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 270us/sample - loss: -6459177.8682 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 281us/sample - loss: -6691733.5645 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 261us/sample - loss: -7144269.7802 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 285us/sample - loss: -6992022.0727 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 269us/sample - loss: -9715740.1769 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 289us/sample - loss: -8526960.4066 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 283us/sample - loss: -9232114.2893 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 278us/sample - loss: -9700568.8033 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 287us/sample - loss: -9681827.3537 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 300us/sample - loss: -9771325.9372 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 267us/sample - loss: -10941803.2942 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 286us/sample - loss: -10398517.1421 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 291us/sample - loss: -11793980.0694 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 301us/sample - loss: -11790017.5455 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 306us/sample - loss: -10907750.7926 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 306us/sample - loss: -12331557.7281 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 270us/sample - loss: -12789464.5186 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 292us/sample - loss: -13403986.9025 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 264us/sample - loss: -15877640.6182 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 298us/sample - loss: -15185490.0380 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 271us/sample - loss: -16548277.8628 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 299us/sample - loss: -15688052.7289 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 274us/sample - loss: -15266179.7752 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 273us/sample - loss: -17530244.7140 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 299us/sample - loss: -16294228.1554 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 295us/sample - loss: -18737414.6380 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 307us/sample - loss: -18167847.1702 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 301us/sample - loss: -21904105.6992 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 312us/sample - loss: -19443080.4661 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 277us/sample - loss: -22608493.5116 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 284us/sample - loss: -20121038.2678 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 286us/sample - loss: -19858205.7587 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 275us/sample - loss: -20319463.5041 - accuracy: 0.0000e+00\n",
      "Train on 2420 samples\n",
      "2420/2420 [==============================] - 1s 295us/sample - loss: -23118365.3521 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "niter=100 #实验重复100次\n",
    "R22=np.zeros(niter)\n",
    "for i in range(niter):\n",
    "    XZ0,XZ1,Y0,Y1= train_test_split(XZ,Y,test_size=0.5,random_state=i) #\n",
    "    model2.fit(XZ0,Y0)\n",
    "    Yhat=regmodel.predict(XZ1)\n",
    "    R22[i]=100.0*(1-np.median((Yhat-Y1)**2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.73"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.median(R22),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用深度学习模型得到的MedSE的中位数是80.73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x7fbc958dc9d0>,\n",
       "  <matplotlib.lines.Line2D at 0x7fbc958dcf10>,\n",
       "  <matplotlib.lines.Line2D at 0x7fbc958f1f50>,\n",
       "  <matplotlib.lines.Line2D at 0x7fbc958eaf10>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x7fbc958cfd10>,\n",
       "  <matplotlib.lines.Line2D at 0x7fbc958ea910>,\n",
       "  <matplotlib.lines.Line2D at 0x7fbc958fc950>,\n",
       "  <matplotlib.lines.Line2D at 0x7fbc958fce50>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x7fbc958cfcd0>,\n",
       "  <matplotlib.lines.Line2D at 0x7fbc958f1990>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x7fbc958eae90>,\n",
       "  <matplotlib.lines.Line2D at 0x7fbc958eae50>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x7fbc958dcf90>,\n",
       "  <matplotlib.lines.Line2D at 0x7fbc95882890>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot((R21,R22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([ 5., 14., 33., 30., 16.,  2.,  0.,  0.,  0.,  0.]),\n",
       "  array([ 0.,  1.,  0.,  0.,  0.,  0.,  0., 10., 53., 36.])],\n",
       " array([71.99192223, 72.99737213, 74.00282203, 75.00827192, 76.01372182,\n",
       "        77.01917172, 78.02462162, 79.03007152, 80.03552142, 81.04097132,\n",
       "        82.04642122]),\n",
       " <a list of 2 Lists of Patches objects>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist((R21,R22))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
